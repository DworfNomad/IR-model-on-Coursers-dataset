{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fb3777",
   "metadata": {},
   "source": [
    "# IR model applied to Coursera dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f4cfef",
   "metadata": {},
   "source": [
    "### Importing essential libraries for data analysis and visualization\n",
    "- **NumPy** for numerical operations in linear algebra\n",
    "- **Pandas** for data manipulation and analysis, including CSV file input/output\n",
    "- **Matplotlib** for basic data visualization\n",
    "- **Seaborn** for statistical data visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd11487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # for data visualization purposes\n",
    "import seaborn as sns # for statistical data visualization\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "ds = pd.read_csv('CourseraDataset-Clean.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d5e0b1",
   "metadata": {},
   "source": [
    "### retrieving the dimension of a Pandas DataFrame shows 8370 rows and 13 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4304491",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257616bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf24d6d",
   "metadata": {},
   "source": [
    "Pandas method ds.info() provides information about the DataFrame's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dede3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9793a6a",
   "metadata": {},
   "source": [
    "The code below aims to identify, count, and display the names of categorical variables in the DataFrame, along with a preview of the first few rows of those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d1bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "\n",
    "categorical = [var for var in ds.columns if ds[var].dtype=='O']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "\n",
    "print('The categorical variables are :\\n\\n', categorical)\n",
    "\n",
    "# view the categorical variables\n",
    "\n",
    "ds[categorical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables\n",
    "\n",
    "ds[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d87b6b",
   "metadata": {},
   "source": [
    "### The code iterates through each categorical variable in the DataFrame ds and prints the frequency counts of unique values for each variable.\n",
    "------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### The output provides a detailed breakdown of the frequency of different values within each categorical variable, offering insights into the distribution of course-related information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view frequency counts of values in categorical variables\n",
    "\n",
    "for var in categorical: \n",
    "    \n",
    "    print(ds[var].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2fe078",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in categorical: \n",
    "    print(ds[var].value_counts() / float(len(ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ea133",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 'Not specified' values in workclass variable with `NaN`\n",
    "\n",
    "ds['Level'].replace('Not specified', np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdf415",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d50f5e",
   "metadata": {},
   "source": [
    "The code **ds[categorical].isnull().sum()** calculates and prints the count of missing values for each categorical variable in the DataFrame ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37baeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb5927",
   "metadata": {},
   "source": [
    "The code **ds['Schedule'].value_counts()** calculates and prints the frequency count of unique values in the 'Schedule' column of the DataFrame ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Schedule'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f2687",
   "metadata": {},
   "source": [
    "*This output below provides information about the diversity of unique labels within each categorical variable. It shows the number of distinct categories for each variable, which can be useful for understanding the variety and granularity of the data in these categorical columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f17cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in categorical:\n",
    "    \n",
    "    print(var, ' contains ', len(ds[var].unique()), ' labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find numerical variables\n",
    "\n",
    "numerical = [var for var in ds.columns if ds[var].dtype!='O']\n",
    "\n",
    "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "\n",
    "print('The numerical variables are :', numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844852b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[numerical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds.drop(['Schedule', 'Course Url'], axis=1)\n",
    "\n",
    "y = ds['Schedule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbc554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eaa679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of X_train and X_test\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ea4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types in X_train\n",
    "\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162fe754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display categorical variables\n",
    "\n",
    "categorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\n",
    "\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display numerical variables\n",
    "\n",
    "numerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\n",
    "\n",
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20224be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print percentage of missing values in the categorical variables in training set\n",
    "\n",
    "X_train[categorical].isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print categorical variables with missing data\n",
    "\n",
    "for col in categorical:\n",
    "    if X_train[col].isnull().mean()>0:\n",
    "        print(col, (X_train[col].isnull().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing categorical variables with most frequent value\n",
    "\n",
    "for ds2 in [X_train, X_test]:\n",
    "    ds2['Level'].fillna(X_train['Level'].mode()[0], inplace=True)    \n",
    "    ds2['Modules'].fillna(X_train['Modules'].mode()[0], inplace=True)\n",
    "    ds2['Instructor'].fillna(X_train['Instructor'].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eff09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables in X_train\n",
    "\n",
    "X_train[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14789f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables in X_test\n",
    "\n",
    "X_test[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in X_train\n",
    "\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in X_test\n",
    "\n",
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print categorical variables\n",
    "\n",
    "categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b7303",
   "metadata": {},
   "source": [
    "*This output provides a view of the first five rows of the specified categorical columns in your dataset, showcasing details about different online courses, including their titles, difficulty levels, learning objectives, skills gained, course modules, instructors, offering institutions, and associated keywords.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6adee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[categorical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import category encoders\n",
    "\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6199537",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.OneHotEncoder(cols=['Course Title', 'Level', 'What you will learn', 'Skill gain', 'Modules', \n",
    "                                 'Instructor', 'Offered By', 'Keyword'])\n",
    "\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b76f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a8e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b43b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396820a1",
   "metadata": {},
   "source": [
    "### This code snippet is performing robust scaling on both the training and test datasets using the RobustScaler. This type of scaling is beneficial when dealing with datasets that may contain outliers, as it uses the robust statistics (median and interquartile range) to scale the features, making the scaling less sensitive to extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b21a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6284c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf342708",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b235b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a Gaussian Naive Bayes classifier on the training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# instantiate the model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "\n",
    "# fit the model\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = gnb.predict(X_train)\n",
    "\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(gnb.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(gnb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6760eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class distribution in test set\n",
    "\n",
    "y_test.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ace36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null accuracy score\n",
    "\n",
    "null_accuracy = (2412/(2412+99))\n",
    "\n",
    "print('Null accuracy score: {0:0.4f}'. format(null_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', cm[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f129c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize confusion matrix with seaborn heatmap\n",
    "\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n",
    "                                 index=['Predict Positive:1', 'Predict Negative:0'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deaf8af",
   "metadata": {},
   "source": [
    "The code snippet print(classification_report(y_test, y_pred)) is using scikit-learn's classification_report function to generate a text-based summary report of the classification performance on the test set. Here's what it typically includes:\n",
    "\n",
    "- Precision: The ratio of correctly predicted positive observations to the total predicted positives. Precision is a measure of the accuracy of positive predictions.\n",
    "\n",
    "- Recall (Sensitivity): The ratio of correctly predicted positive observations to the all observations in the actual class. Recall measures the ability of the model to capture all the relevant cases.\n",
    "\n",
    "- F1-Score: The weighted average of precision and recall. It is a metric that combines both precision and recall into a single value.\n",
    "\n",
    "- Support: The number of actual occurrences of the class in the specified dataset.\n",
    "\n",
    "- Accuracy: The ratio of correctly predicted observation to the total observations. It is a measure of overall correctness.\n",
    "\n",
    "The output of classification_report gives you insights into the model's performance for each class, as well as an overall summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334460a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e3bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification accuracy\n",
    "\n",
    "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688070e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification error\n",
    "\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification error : {0:0.4f}'.format(classification_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d038f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print precision score\n",
    "\n",
    "precision = TP / float(TP + FP)\n",
    "\n",
    "\n",
    "print('Precision : {0:0.4f}'.format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ddf830",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP / float(TP + FN)\n",
    "\n",
    "print('Recall or Sensitivity : {0:0.4f}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6682ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_rate = TP / float(TP + FN)\n",
    "\n",
    "\n",
    "print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate = FP / float(FP + TN)\n",
    "\n",
    "\n",
    "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69390f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print('Specificity : {0:0.4f}'.format(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fdd048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 predicted probabilities of two classes- 0 and 1\n",
    "\n",
    "y_pred_prob = gnb.predict_proba(X_test)[0:10]\n",
    "\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the probabilities in dataframe\n",
    "\n",
    "y_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['Prob of Hands-on learning', 'Prob of Flexible learning'])\n",
    "\n",
    "y_pred_prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77459fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 predicted probabilities for class 1 - Probability of >50K\n",
    "\n",
    "gnb.predict_proba(X_test)[0:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f182dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 - Probability of >50K\n",
    "\n",
    "y_pred1 = gnb.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91868cc",
   "metadata": {},
   "source": [
    "## Plotting histogram of predicted probabilities of Flexible learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15ad65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot histogram of predicted probabilities\n",
    "\n",
    "\n",
    "# adjust the font size \n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "\n",
    "# plot histogram with 10 bins\n",
    "plt.hist(y_pred1, bins = 10)\n",
    "\n",
    "\n",
    "# set the title of predicted probabilities\n",
    "plt.title('Histogram of predicted probabilities of Flexible learning')\n",
    "\n",
    "\n",
    "# set the x-axis limit\n",
    "plt.xlim(0,1)\n",
    "\n",
    "\n",
    "# set the title\n",
    "plt.xlabel('Predicted probabilities of Flexible learning')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec328b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC Curve\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred1, pos_label = 'Flexible schedule')\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.title('ROC curve for Gaussian Naive Bayes Classifier for Predicting Hands-on learning Schedule')\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a530ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ROC AUC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC = roc_auc_score(y_test, y_pred1)\n",
    "\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cross-validated ROC AUC \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "Cross_validated_ROC_AUC = cross_val_score(gnb, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
    "\n",
    "print('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086cac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying 10-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(gnb, X_train, y_train, cv = 10, scoring='accuracy')\n",
    "\n",
    "print('Cross-validation scores:{}'.format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab988338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Average cross-validation score\n",
    "\n",
    "print('Average cross-validation score: {:.4f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d8080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
